{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FunctionGemma Fine-tune v·ªõi Unsloth\n",
        "\n",
        "Train AI quy·∫øt ƒë·ªãnh: `plant(plant_type, row, col)` ho·∫∑c `wait()`\n",
        "\n",
        "**∆Øu ƒëi·ªÉm Unsloth:**\n",
        "- Nhanh h∆°n 2-5x so v·ªõi HuggingFace\n",
        "- √çt VRAM h∆°n (ch·∫°y ƒë∆∞·ª£c tr√™n T4 free)\n",
        "- H·ªó tr·ª£ FunctionGemma 270M native\n",
        "\n",
        "**Output:** OpenVINO IR format cho inference nhanh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. C√†i ƒë·∫∑t Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install openvino optimum[openvino] -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Upload Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "print(\"Upload training_data.json...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "with open(filename, 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"\\n‚úì Loaded {len(raw_data)} samples\")\n",
        "stats = {}\n",
        "for s in raw_data:\n",
        "    stats[s['action']] = stats.get(s['action'], 0) + 1\n",
        "print(f\"  Actions: {stats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load FunctionGemma v·ªõi Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/functiongemma-270m-it\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=False,   # Full precision cho model nh·ªè\n",
        "    load_in_16bit=True,   # 16bit LoRA\n",
        "    full_finetuning=True, # Full finetune v√¨ model nh·ªè\n",
        ")\n",
        "\n",
        "print(f\"‚úì Model loaded: {model.config._name_or_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plant(plant_type: str, row: int, col: int):\n",
        "    \"\"\"\n",
        "    Plant a plant at grid position.\n",
        "\n",
        "    Args:\n",
        "        plant_type: Type of plant (pea_shooter, sunflower, wall_nut, cherry_bomb, etc)\n",
        "        row: Row index 0-4 (0=top, 4=bottom)\n",
        "        col: Column index 0-8 (0=left, 8=right)\n",
        "\n",
        "    Returns:\n",
        "        result: Action result\n",
        "    \"\"\"\n",
        "    return {\"result\": \"planted\"}\n",
        "\n",
        "def wait():\n",
        "    \"\"\"\n",
        "    Wait and do nothing. Use when seed is on cooldown or no good action available.\n",
        "\n",
        "    Returns:\n",
        "        result: Action result\n",
        "    \"\"\"\n",
        "    return {\"result\": \"waiting\"}\n",
        "\n",
        "TOOLS = [plant, wait]\n",
        "print(\"‚úì Tools defined:\", [f.__name__ for f in TOOLS])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "SYSTEM_MSG = \"\"\"PvZ bot. Choose action based on game state.\n",
        "- PLANTS: planted plants (type,row,col)\n",
        "- ZOMBIES: zombies (type,row,col)\n",
        "- SEEDS: seed packets (type,status: ready/cooldown)\n",
        "Plant when seed ready and position valid. Wait when cooldown or no threat.\"\"\"\n",
        "\n",
        "def format_for_training(sample):\n",
        "    \"\"\"Format sample cho FunctionGemma chat template\"\"\"\n",
        "    action = sample[\"action\"]\n",
        "    args = sample.get(\"arguments\", {})\n",
        "\n",
        "    if action == \"plant\":\n",
        "        tool_call = {\"type\": \"function\", \"function\": {\"name\": \"plant\", \"arguments\": args}}\n",
        "    else:\n",
        "        tool_call = {\"type\": \"function\", \"function\": {\"name\": \"wait\", \"arguments\": {}}}\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"developer\", \"content\": SYSTEM_MSG},\n",
        "        {\"role\": \"user\", \"content\": sample[\"game_state\"]},\n",
        "        {\"role\": \"assistant\", \"tool_calls\": [tool_call]},\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tools=TOOLS,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Shuffle v√† format\n",
        "random.shuffle(raw_data)\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "dataset = dataset.map(format_for_training, remove_columns=dataset.features)\n",
        "\n",
        "# Split train/test\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True)\n",
        "print(f\"‚úì Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
        "print(f\"\\nSample:\\n{dataset['train'][0]['text'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training v·ªõi Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "num_samples = len(raw_data)\n",
        "# T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh epochs d·ª±a tr√™n s·ªë samples\n",
        "epochs = max(3, min(20, 500 // num_samples))\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"pvz_gemma\",\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"Training {num_samples} samples for {epochs} epochs...\")\n",
        "print(f\"Batch size: 4 x 2 = 8 effective\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "print(f\"\\n‚úì Training complete in {trainer_stats.metrics['train_runtime']:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_tool_call(text):\n",
        "    \"\"\"Parse FunctionGemma output\"\"\"\n",
        "    match = re.search(r\"<start_function_call>call:(\\w+)\\{(.*?)\\}<end_function_call>\", text, re.DOTALL)\n",
        "    if not match:\n",
        "        return None\n",
        "    name = match.group(1)\n",
        "    args_str = match.group(2)\n",
        "    args = {}\n",
        "    for k, v in re.findall(r\"(\\w+):([^,}]+)\", args_str):\n",
        "        v = v.strip()\n",
        "        try:\n",
        "            args[k] = int(v)\n",
        "        except:\n",
        "            args[k] = v\n",
        "    return {\"name\": name, \"arguments\": args}\n",
        "\n",
        "def test_bot(game_state):\n",
        "    messages = [\n",
        "        {\"role\": \"developer\", \"content\": SYSTEM_MSG},\n",
        "        {\"role\": \"user\", \"content\": game_state},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, tools=TOOLS, add_generation_prompt=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    out = model.generate(\n",
        "        **inputs.to(model.device),\n",
        "        max_new_tokens=64,\n",
        "        top_k=64, top_p=0.95, temperature=1.0,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    output = tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=False)\n",
        "    return extract_tool_call(output)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"TEST PVZ BOT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "test_cases = [\n",
        "    \"PLANTS:[]. ZOMBIES:[]. SEEDS:[(pea_shooter,ready)]\",\n",
        "    \"PLANTS:[(pea_shooter,2,0)]. ZOMBIES:[(zombie,2,7)]. SEEDS:[(pea_shooter,cooldown)]\",\n",
        "    \"PLANTS:[(pea_shooter,2,0)]. ZOMBIES:[(zombie,1,6)]. SEEDS:[(pea_shooter,ready)]\",\n",
        "    \"PLANTS:[]. ZOMBIES:[(zombie,0,8),(zombie,4,7)]. SEEDS:[(pea_shooter,ready),(sunflower,ready)]\",\n",
        "]\n",
        "\n",
        "for t in test_cases:\n",
        "    result = test_bot(t)\n",
        "    print(f\"\\nüì• {t}\")\n",
        "    print(f\"üì§ {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save v·ªõi Unsloth (nhanh h∆°n)\n",
        "model.save_pretrained(\"pvz_gemma_pytorch\")\n",
        "tokenizer.save_pretrained(\"pvz_gemma_pytorch\")\n",
        "print(\"‚úì PyTorch model saved to pvz_gemma_pytorch/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export to OpenVINO IR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optimum.intel import OVModelForCausalLM\n",
        "\n",
        "print(\"Converting to OpenVINO IR format...\")\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    \"pvz_gemma_pytorch\",\n",
        "    export=True,\n",
        "    compile=False\n",
        ")\n",
        "ov_model.save_pretrained(\"pvz_gemma_openvino\")\n",
        "tokenizer.save_pretrained(\"pvz_gemma_openvino\")\n",
        "print(\"‚úì OpenVINO model saved to pvz_gemma_openvino/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test OpenVINO Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from optimum.intel import OVModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\"pvz_gemma_openvino\")\n",
        "ov_tokenizer = AutoTokenizer.from_pretrained(\"pvz_gemma_openvino\")\n",
        "\n",
        "def test_ov(game_state):\n",
        "    messages = [\n",
        "        {\"role\": \"developer\", \"content\": SYSTEM_MSG},\n",
        "        {\"role\": \"user\", \"content\": game_state},\n",
        "    ]\n",
        "    inputs = ov_tokenizer.apply_chat_template(\n",
        "        messages, tools=TOOLS, add_generation_prompt=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    out = ov_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        pad_token_id=ov_tokenizer.eos_token_id\n",
        "    )\n",
        "    output = ov_tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=False)\n",
        "    return extract_tool_call(output)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST OPENVINO MODEL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for t in test_cases:\n",
        "    result = test_ov(t)\n",
        "    print(f\"\\nüì• {t}\")\n",
        "    print(f\"üì§ {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r pvz_gemma_openvino.zip pvz_gemma_openvino/\n",
        "\n",
        "print(\"\\n‚úì Model ready for download:\")\n",
        "print(\"  - pvz_gemma_openvino.zip (OpenVINO IR format)\")\n",
        "\n",
        "# Show size\n",
        "!ls -lh pvz_gemma_openvino.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('pvz_gemma_openvino.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Save to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment ƒë·ªÉ push l√™n HuggingFace\n",
        "# from huggingface_hub import login\n",
        "# login()\n",
        "# model.push_to_hub(\"your-username/pvz-gemma-bot\")\n",
        "# tokenizer.push_to_hub(\"your-username/pvz-gemma-bot\")"
      ]
    }
  ]
}
